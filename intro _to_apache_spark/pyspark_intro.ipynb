{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIpgWlms8eYn",
        "outputId": "695ba8cc-6bb9-4c3f-98b7-89be4adf38e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=df506f54732c8d84621764318e15be4813887772861e6a261a297f372c07dc97\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ],
      "source": [
        "# Installing required packages\n",
        "!pip install pyspark\n",
        "!pip install findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "hR7DPzgH8xbs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PySpark is the Spark API for Python. In this lab, we use PySpark to initialize the spark context.\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "86gY--MV83Dy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating spark session and context\n"
      ],
      "metadata": {
        "id": "aX4sJj1c853A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a spark context class\n",
        "sc = SparkContext()\n",
        "\n",
        "# Creating a spark session\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Python Spark DataFrames basic example\") \\\n",
        "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "Lq9zausL8_M8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize spark session"
      ],
      "metadata": {
        "id": "u1C5_Wqi9B3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'spark' in locals() and isinstance(spark, SparkSession):\n",
        "    print(\"SparkSession is active and ready to use.\")\n",
        "else:\n",
        "    print(\"SparkSession is not active. Please create a SparkSession.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuz4h8-59FlH",
        "outputId": "ab247a1e-e76d-48cf-d912-6ebef022139f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SparkSession is active and ready to use.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create an RDD"
      ],
      "metadata": {
        "id": "Lp93eqjt9OLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = range(1,30)\n",
        "# print first element of iterator\n",
        "print(data[0])\n",
        "len(data)\n",
        "xrangeRDD = sc.parallelize(data, 4)\n",
        "\n",
        "# this will let us know that we created an RDD\n",
        "xrangeRDD"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0inlsdj9Kkl",
        "outputId": "4219ad6f-c368-428c-b1ae-89fc96d15563"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PythonRDD[1] at RDD at PythonRDD.scala:53"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformations"
      ],
      "metadata": {
        "id": "i-MTZbbW9ZBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subRDD = xrangeRDD.map(lambda x: x-1)\n",
        "filteredRDD = subRDD.filter(lambda x : x<10)"
      ],
      "metadata": {
        "id": "2r0JfIOV9VpL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Actions"
      ],
      "metadata": {
        "id": "iW86sgMh-GmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(filteredRDD.collect())\n",
        "filteredRDD.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfJAFx_y-D-K",
        "outputId": "9c3c9376-fdc2-407a-d04e-3ae11d6a6285"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Caching data"
      ],
      "metadata": {
        "id": "V1ZJT5if-NWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "test = sc.parallelize(range(1,50000),4)\n",
        "test.cache()\n",
        "\n",
        "t1 = time.time()\n",
        "# first count will trigger evaluation of count *and* cache\n",
        "count1 = test.count()\n",
        "dt1 = time.time() - t1\n",
        "print(\"dt1: \", dt1)\n",
        "\n",
        "\n",
        "t2 = time.time()\n",
        "# second count operates on cached data only\n",
        "count2 = test.count()\n",
        "dt2 = time.time() - t2\n",
        "print(\"dt2: \", dt2)\n",
        "\n",
        "#test.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ARG8EKP-Mbo",
        "outputId": "11ea3f8c-fc74-4b0c-857c-035f28fbf3ec"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dt1:  3.297896146774292\n",
            "dt2:  1.2413139343261719\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating dataframe"
      ],
      "metadata": {
        "id": "Sj5am9Y9-ZVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the data first into a local `people.json` file\n",
        "!curl https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/people.json >> people.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qX83e9Gn-cP4",
        "outputId": "950c07c7-bc29-451a-d75d-670aab815fe2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100    73  100    73    0     0    113      0 --:--:-- --:--:-- --:--:--   113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the dataset into a spark dataframe using the `read.json()` function\n",
        "df = spark.read.json(\"people.json\").cache()"
      ],
      "metadata": {
        "id": "ABjPj5WZ-gaX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the dataframe as well as the data schema\n",
        "df.show()\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4f0LjEJ-lSa",
        "outputId": "843820be-a730-4c2e-f196-ec804e5cae5b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-------+\n",
            "| age|   name|\n",
            "+----+-------+\n",
            "|NULL|Michael|\n",
            "|  30|   Andy|\n",
            "|  19| Justin|\n",
            "+----+-------+\n",
            "\n",
            "root\n",
            " |-- age: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Register the DataFrame as a SQL temporary view\n",
        "df.createTempView(\"people\")"
      ],
      "metadata": {
        "id": "tT0HckqI-nlF"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploring the dataframe"
      ],
      "metadata": {
        "id": "Qu-mmroF-q5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select and show basic data columns\n",
        "\n",
        "df.select(\"name\").show()\n",
        "df.select(df[\"name\"]).show()\n",
        "spark.sql(\"SELECT name FROM people\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLZt8t6q-ugk",
        "outputId": "acb9d133-2cf2-4cc3-860c-ac78767731de"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+\n",
            "|   name|\n",
            "+-------+\n",
            "|Michael|\n",
            "|   Andy|\n",
            "| Justin|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|   name|\n",
            "+-------+\n",
            "|Michael|\n",
            "|   Andy|\n",
            "| Justin|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|   name|\n",
            "+-------+\n",
            "|Michael|\n",
            "|   Andy|\n",
            "| Justin|\n",
            "+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform basic filtering\n",
        "\n",
        "df.filter(df[\"age\"] > 21).show()\n",
        "spark.sql(\"SELECT age, name FROM people WHERE age > 21\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhZye82V-x9-",
        "outputId": "4ed601a3-d1e0-47c4-e84b-ccb9241fd4f0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+\n",
            "|age|name|\n",
            "+---+----+\n",
            "| 30|Andy|\n",
            "+---+----+\n",
            "\n",
            "+---+----+\n",
            "|age|name|\n",
            "+---+----+\n",
            "| 30|Andy|\n",
            "+---+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perfom basic aggregation of data\n",
        "\n",
        "df.groupBy(\"age\").count().show()\n",
        "spark.sql(\"SELECT age, COUNT(age) as count FROM people GROUP BY age\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fizYR4HS-0Dz",
        "outputId": "f9c8d8a6-42e3-43e5-ee9e-37f7b0c09bb4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+\n",
            "| age|count|\n",
            "+----+-----+\n",
            "|  19|    1|\n",
            "|NULL|    1|\n",
            "|  30|    1|\n",
            "+----+-----+\n",
            "\n",
            "+----+-----+\n",
            "| age|count|\n",
            "+----+-----+\n",
            "|  19|    1|\n",
            "|NULL|    0|\n",
            "|  30|    1|\n",
            "+----+-----+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}